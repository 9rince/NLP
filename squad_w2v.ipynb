{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\princ3\\anaconda3\\envs\\tfconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob   # for smarter sentence parsing !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_pickle('./input/squad_contxt.pkl')\n",
    "df_q = pd.read_pickle('./input/squad_qas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_no</th>\n",
       "      <th>context_no</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles was born in Houston, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Beyoncé attended St. Mary's Elementary School ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     title title_no context_no  \\\n",
       "1  Beyoncé        1          1   \n",
       "2  Beyoncé        1          2   \n",
       "3  Beyoncé        1          3   \n",
       "4  Beyoncé        1          4   \n",
       "5  Beyoncé        1          5   \n",
       "\n",
       "                                             context  \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "2  Following the disbandment of Destiny's Child i...  \n",
       "3  A self-described \"modern-day feminist\", Beyonc...  \n",
       "4  Beyoncé Giselle Knowles was born in Houston, T...  \n",
       "5  Beyoncé attended St. Mary's Elementary School ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = df_p['context']\n",
    "data_q = df_q['Question']\n",
    "\n",
    "with open(\"Output.txt\", \"w\",encoding=\"utf-8\") as text_file:\n",
    "    for i in data_p:\n",
    "        print(i, file=text_file)\n",
    "    for j in data_q:\n",
    "        print(j, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"Output.txt\",encoding = \"utf-8\")\n",
    "t=file.read()\n",
    "file.close()\n",
    "TB = TextBlob(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 222825/222825 [00:48<00:00, 4622.22it/s]\n"
     ]
    }
   ],
   "source": [
    "sentx = []\n",
    "# charx = []\n",
    "for i in tqdm(TB.sentences):\n",
    "    sentx.append(i.words)\n",
    "#     charx.append(list(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=118411, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentx, min_count=1,size = 100)\n",
    "# model_c = Word2Vec(charx,min_count=1,size = 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "# chars = list(model_c.wv.vocab)\n",
    "# words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12980412  0.3605678  -0.01596252 -0.06291172  0.06276257 -0.06195724\n",
      "  0.07557134  0.03405358  0.18794608 -0.2485839   0.06394231 -0.07699699\n",
      "  0.29259315  0.10406041 -0.02328249  0.20798084 -0.1586454   0.12113716\n",
      "  0.11076957  0.0141861  -0.04145353  0.25242615 -0.39934152  0.04481733\n",
      " -0.02816148 -0.00081677 -0.1529618  -0.11582003 -0.19349739  0.01588384\n",
      " -0.05536219 -0.02929357  0.25740042 -0.03635935  0.23543362  0.31334928\n",
      "  0.06694116 -0.29028347  0.08165566 -0.15295206  0.024217    0.18920892\n",
      "  0.02500139  0.05276724 -0.07882727 -0.13571188  0.1553474   0.05794151\n",
      " -0.02848434 -0.09018485  0.08894     0.04969027 -0.04892686 -0.13753386\n",
      "  0.17022897 -0.10139967  0.06716244 -0.21422102 -0.13599916 -0.1508197\n",
      "  0.05016415 -0.07115296  0.17733529  0.08437354 -0.02782485 -0.02458114\n",
      " -0.18586665 -0.205382   -0.35351875  0.10938178 -0.18322615 -0.18618435\n",
      " -0.05863042 -0.03727883  0.14963213  0.09771223  0.01456377  0.17884362\n",
      " -0.06673063 -0.02126634  0.18336488  0.04159831  0.01921893  0.01103984\n",
      "  0.09043149 -0.01546779 -0.09660093  0.02315422  0.02716192 -0.19526693\n",
      " -0.03507322 -0.07932998 -0.16679579 -0.09584435  0.00912627  0.0492013\n",
      " -0.03503934 -0.28225708  0.02457335  0.02982248]\n"
     ]
    }
   ],
   "source": [
    "print(model['Glastonbury'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_c.save('model_c.bin')\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Word2Vec.load('model_c.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "pyplot.figure(figsize=(20,20))\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model_c[model_c.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "print(chars)\n",
    "for i, word in enumerate(chars):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
