{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\princ3\\anaconda3\\envs\\tfconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob   # for smarter sentence parsing !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_pickle('./input/squad_contxt.pkl')\n",
    "df_q = pd.read_pickle('./input/squad_qas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_no</th>\n",
       "      <th>context_no</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles was born in Houston, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Beyoncé attended St. Mary's Elementary School ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     title title_no context_no  \\\n",
       "1  Beyoncé        1          1   \n",
       "2  Beyoncé        1          2   \n",
       "3  Beyoncé        1          3   \n",
       "4  Beyoncé        1          4   \n",
       "5  Beyoncé        1          5   \n",
       "\n",
       "                                             context  \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "2  Following the disbandment of Destiny's Child i...  \n",
       "3  A self-described \"modern-day feminist\", Beyonc...  \n",
       "4  Beyoncé Giselle Knowles was born in Houston, T...  \n",
       "5  Beyoncé attended St. Mary's Elementary School ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = df_p['context']\n",
    "data_q = df_q['Question']\n",
    "\n",
    "# with open(\"Output.txt\", \"w\",encoding=\"utf-8\") as text_file:\n",
    "#     for i in data_p:\n",
    "#         print(i, file=text_file)\n",
    "#     for j in data_q:\n",
    "#         print(j, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"Output.txt\",encoding = \"utf-8\")\n",
    "t=file.read()\n",
    "file.close()\n",
    "TB = TextBlob(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 222825/222825 [01:16<00:00, 2903.54it/s]\n"
     ]
    }
   ],
   "source": [
    "sentx = []\n",
    "# charx = []\n",
    "for i in tqdm(TB.sentences):\n",
    "    sentx.append(TextBlob(str(i)+\" E-O-S\").words)\n",
    "#     charx.append(list(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=118412, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentx, min_count=1,size = 100)\n",
    "# model_c = Word2Vec(charx,min_count=1,size = 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "# chars = list(model_c.wv.vocab)\n",
    "# words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3348689e+00 -5.1987833e-01  1.1996955e+00  1.7167928e+00\n",
      "  1.2624527e+00  3.5665455e-01  1.8161983e+00 -5.5029267e-01\n",
      " -2.5469264e-01 -8.4256107e-01  4.1367391e-01  1.0602208e+00\n",
      " -6.5409094e-01  6.1337668e-01 -3.6125717e-01  8.7640041e-01\n",
      " -1.8517588e+00  1.2860365e-01 -1.3695477e+00 -3.7261009e-01\n",
      " -1.2698674e+00  8.4018326e-01  9.7453192e-02 -5.6512270e-02\n",
      " -1.7297770e-01 -1.8749963e+00 -3.0702657e-01  4.9244919e-01\n",
      "  3.5744143e-01  1.6949346e+00 -4.8316109e-01 -1.0253336e+00\n",
      "  1.0881406e-01 -1.4007972e+00  3.8877684e-01  6.0365695e-01\n",
      " -3.0418730e-01 -4.0966460e-01 -1.0525830e+00  7.2243720e-01\n",
      "  3.8557265e-02 -5.7863485e-02 -5.8625752e-01  7.1322244e-01\n",
      "  4.9919659e-01  1.8702011e+00 -1.2974203e+00 -5.3107822e-01\n",
      " -7.5773168e-01 -1.7197961e-01 -1.3642269e-01  6.8241441e-01\n",
      " -2.2422607e-01 -3.6814782e-01  1.0110376e+00 -6.5529716e-01\n",
      " -3.5564095e-01 -3.8315096e-01 -6.2185639e-01 -6.4021057e-01\n",
      " -1.0983258e+00 -2.5793639e-01 -5.5750954e-01  5.3401887e-01\n",
      " -1.3483018e+00  9.8279852e-01 -1.0731171e+00  1.1756046e+00\n",
      " -3.6260042e-02  4.3023840e-01 -7.3175216e-01 -5.1374656e-01\n",
      " -1.1320180e+00  1.8015528e+00 -6.4848453e-01 -1.5748743e+00\n",
      " -6.9713664e-01  1.0427014e+00  5.7205915e-01  3.5624605e-01\n",
      " -3.6406893e-01  7.5087684e-01 -7.7183849e-01 -2.1798559e-01\n",
      " -6.1347757e-02  1.2067814e+00 -9.8114121e-01 -1.2194697e+00\n",
      "  7.1134084e-01 -1.3056481e-01  8.6162186e-01 -6.7360109e-01\n",
      "  2.7320189e+00 -2.8636700e-01  8.2168448e-01 -5.7630575e-01\n",
      " -7.1863502e-01 -1.7593304e-03 -6.0982007e-01  8.9666140e-01]\n"
     ]
    }
   ],
   "source": [
    "print(model['E-O-S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_c.save('model_c.bin')\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Word2Vec.load('model_c.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "pyplot.figure(figsize=(20,20))\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model_c[model_c.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "print(chars)\n",
    "for i, word in enumerate(chars):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
