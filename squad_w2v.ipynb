{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\princ3\\anaconda3\\envs\\tfconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob   # for smarter sentence parsing !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_pickle('./input/squad_contxt.pkl')\n",
    "df_q = pd.read_pickle('./input/squad_qas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_no</th>\n",
       "      <th>context_no</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles was born in Houston, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Beyoncé attended St. Mary's Elementary School ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     title title_no context_no  \\\n",
       "1  Beyoncé        1          1   \n",
       "2  Beyoncé        1          2   \n",
       "3  Beyoncé        1          3   \n",
       "4  Beyoncé        1          4   \n",
       "5  Beyoncé        1          5   \n",
       "\n",
       "                                             context  \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "2  Following the disbandment of Destiny's Child i...  \n",
       "3  A self-described \"modern-day feminist\", Beyonc...  \n",
       "4  Beyoncé Giselle Knowles was born in Houston, T...  \n",
       "5  Beyoncé attended St. Mary's Elementary School ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = df_p['context']\n",
    "data_q = df_q['Question']\n",
    "\n",
    "# with open(\"Output.txt\", \"w\",encoding=\"utf-8\") as text_file:\n",
    "#     for i in data_p:\n",
    "#         print(i, file=text_file)\n",
    "#     for j in data_q:\n",
    "#         print(j, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"Output.txt\",encoding = \"utf-8\")\n",
    "t=file.read()\n",
    "file.close()\n",
    "TB = TextBlob(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 222825/222825 [02:32<00:00, 1464.10it/s]\n"
     ]
    }
   ],
   "source": [
    "sentx = []\n",
    "# charx = []\n",
    "for i in tqdm(TB.sentences):\n",
    "    sentx.append(i.words)\n",
    "#     charx.append(list(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=118411, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentx, min_count=1,size = 100)\n",
    "# model_c = Word2Vec(charx,min_count=1,size = 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "# chars = list(model_c.wv.vocab)\n",
    "# words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.27706742e-01  1.35984123e+00  1.54926252e+00 -5.90608180e-01\n",
      "  4.46737200e-01 -1.79827943e-01 -9.71029162e-01  1.74888146e+00\n",
      " -2.46905923e-01 -6.84290588e-01  1.59009445e+00 -8.88351262e-01\n",
      " -8.25236738e-01 -1.66100338e-01  7.63681978e-02  2.12135959e+00\n",
      "  4.29004505e-02 -2.78567046e-01  5.62600553e-01 -9.64628458e-01\n",
      "  1.05372362e-01  8.22525561e-01  5.69588125e-01  1.36900890e+00\n",
      "  1.20261168e+00  9.14283097e-02  7.77260125e-01  1.04015604e-01\n",
      "  8.64619970e-01 -3.04902822e-01 -1.01742017e+00  1.11678779e+00\n",
      " -2.35268041e-01 -2.60690260e+00  5.69940090e-01 -1.45648146e+00\n",
      "  2.14877653e+00 -1.54757631e+00  8.83437216e-01  7.66737044e-01\n",
      "  1.04357767e+00 -6.43730443e-03  4.94171113e-01  1.03242099e+00\n",
      "  1.95459890e+00 -1.01320279e+00 -3.01374853e-01  3.10410643e+00\n",
      " -1.67782664e+00  2.29463959e+00 -1.02715850e+00  9.47456121e-01\n",
      " -9.81112659e-01 -1.41566738e-01 -3.72541428e-01  1.39144826e+00\n",
      "  3.42989743e-01  7.49579165e-03  1.30427158e+00  1.09187996e+00\n",
      "  5.37676454e-01  9.19764698e-01 -3.50965440e-01  1.54550120e-01\n",
      " -5.37673891e-01  6.47894859e-01 -9.18522239e-01 -7.05749035e-01\n",
      "  8.13901961e-01 -8.34878504e-01  3.99753988e-01  4.71682660e-02\n",
      " -1.63572061e+00  9.65793133e-01 -2.00954735e-01 -8.41516554e-01\n",
      " -1.04323156e-01  7.49374747e-01 -4.30138379e-01 -1.21402562e+00\n",
      " -1.22760677e+00  1.42131889e+00  1.18866134e+00  1.88167930e+00\n",
      "  4.32531297e-01 -4.20947880e-01  3.80174428e-01  2.55203783e-03\n",
      "  6.37738764e-01 -3.54754925e-01  1.66045234e-01  2.05457163e+00\n",
      "  5.93798459e-01  1.56225055e-01 -3.45663875e-01  4.30946797e-01\n",
      " -2.07229924e+00  2.38718390e-01  6.33854449e-01  1.08642983e+00]\n"
     ]
    }
   ],
   "source": [
    "print(model['and'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_c.save('model_c.bin')\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Word2Vec.load('model_c.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "pyplot.figure(figsize=(20,20))\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model_c[model_c.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "print(chars)\n",
    "for i, word in enumerate(chars):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
