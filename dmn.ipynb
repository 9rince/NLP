{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = Word2Vec.load('model.bin')    # word embedding\n",
    "# char_vec = Word2Vec.load('model_c.bin')  # charector embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contxt = pd.read_pickle('./input/squad_contxt.pkl') # context\n",
    "df_qas = pd.read_pickle('./input/squad_qas.pkl')       # question and answers\n",
    "df_qas = df_qas[df_qas.is_impossible != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_contxt),len(df_qas))\n",
    "max_c_word_length = 0\n",
    "min_c_word_length = 1000\n",
    "max_c_snt_length = 0\n",
    "min_c_snt_length = 1000\n",
    "p_word_length = []\n",
    "p_snt_length = []\n",
    "ran = 1\n",
    "for i in tqdm(df_contxt['context']):\n",
    "    w_len = 0\n",
    "    l = len(TextBlob(i).sentences)\n",
    "    if l < min_c_snt_length:\n",
    "        min_c_snt_length = l\n",
    "    if l > max_c_snt_length:\n",
    "        max_c_snt_length = l   \n",
    "    p_snt_length.append(l)\n",
    "    for j in TextBlob(i).sentences:\n",
    "        k = len((j+\" E-O-S\").words)\n",
    "        w_len += k\n",
    "    p_word_length.append(w_len)\n",
    "    if w_len > max_c_word_length:\n",
    "        max_c_word_length = w_len\n",
    "    if w_len < min_c_word_length:\n",
    "        min_c_word_length = w_len\n",
    "#     print(w_len,df_contxt['word_len'][ran])\n",
    "    ran += 1\n",
    "print(min_c_word_length,max_c_word_length)\n",
    "print(min_c_snt_length,max_c_snt_length)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_contxt),len(p_word_length))\n",
    "df_contxt['snt_len'] = p_snt_length\n",
    "df_contxt['word_len'] = p_word_length\n",
    "\n",
    "df_contxt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.hist(p_word_length,bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_snt_length,bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_q_word_length = 0\n",
    "min_q_word_length = 1000\n",
    "max_q_snt_length = 0\n",
    "min_q_snt_length = 1000\n",
    "q_word_len = []\n",
    "q_snt_len = []\n",
    "# ran = 0\n",
    "for i in tqdm(df_qas[\"Question\"]):\n",
    "    q_word_len.append(len(TextBlob(i).words))\n",
    "    q_snt_len.append(len(TextBlob(i).sentences))\n",
    "    if len(TextBlob(i).sentences)>max_q_snt_length:\n",
    "        max_q_snt_length = len(TextBlob(i).sentences)\n",
    "    if len(TextBlob(i).sentences)<min_q_snt_length:\n",
    "        min_q_snt_length = len(TextBlob(i).sentences)\n",
    "    if len(TextBlob(i).words)<min_q_word_length:\n",
    "        min_q_word_length = len(TextBlob(i).words)\n",
    "    if len(TextBlob(i).words)>max_q_word_length:\n",
    "        max_q_word_length = len(TextBlob(i).words)\n",
    "df_qas['word_len'] = q_word_len\n",
    "df_qas['snt_len'] = q_snt_len\n",
    "#     if len(TextBlob(i).sentenses) >1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_q_snt_length,max_q_snt_length)\n",
    "print(min_q_word_length,max_q_word_length)\n",
    "df_qas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(q_snt_len,bins = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(q_word_len,bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contxt = df_contxt[df_contxt.word_len <= 200 ] # removing paras with more than 200 words\n",
    "df_qas = df_qas[df_qas.snt_len ==1] # removing questions with more than one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_contxt),len(df_qas)\n",
    "# df_contxt['context_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_qas['context_no']:\n",
    "    if i not in df_contxt['context_no']:\n",
    "        #print('gotcha !',i)\n",
    "        df_qas = df_qas[df_qas.context_no != i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prep_data():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, word_e = word_vec):\n",
    "        \n",
    "        self.contxt = pd.read_pickle('./input/squad_contxt.pkl')\n",
    "        self.qas = pd.read_pickle('./input/squad_qas.pkl')\n",
    "#         self.char_e = char_e\n",
    "        self.word_e = word_e\n",
    "\n",
    "    def C2V(self,q_no):   # this function converts context 2 vector with <EOS> tag\n",
    "        tb_p = TextBlob(self.context['Context'][self.qas['context_no'][q_no]])#TextBlob(passage)\n",
    "        tb_q = TextBlob(self.qas['Question'][q_no])\n",
    "        p2v = []\n",
    "        q2v = []\n",
    "        eos_tag = []\n",
    "        c = 0\n",
    "        for i in tb_p.sentences:\n",
    "            words = TextBlob(str(i)+\" E-O-S\").words\n",
    "            for j in words:\n",
    "                p2v.append(self.word_e[j])\n",
    "                if j=='E-O-S':\n",
    "                    eos_tag.append(c)\n",
    "            c += 1\n",
    "        for k in (str(tb_q)+\" E-O-S\").words:\n",
    "            q2v.append(self.word_e[k])\n",
    "            \n",
    "        return np.array(p2v),np.array(q2v),eos_tag\n",
    "    \n",
    "#     def QA2V(self,q_no):  # this function converts question 2 vector\n",
    "#         tb_a = TextBlob(self)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gru_models():\n",
    "    \"\"\"\n",
    "        It's a modified version of normal GRU\n",
    "    \"\"\"\n",
    "    def __init__(self,time_steps,init_state,input_size,tag,\n",
    "                 output_size,question=None,final_mem=None,input_seq=None):\n",
    "        \n",
    "        # inputs required for sequential memory vectors e_i \n",
    "        self.time_steps = time_steps              # equal to max no concepts\n",
    "        self.init_state = init_state              # initial secondary input ideally zero tensor\n",
    "        self.input_seq = input_seq                # input seq to be evaluated\n",
    "        self.input_size = input_size              # size of input vector \n",
    "        self.output_size = output_size            # size of output required\n",
    "        \n",
    "        # inputs required for Answer modules\n",
    "        self.question = question\n",
    "        self.final_mem = final_mem\n",
    "        self.word_weight = tf.get_variable(\"word_w_{}\".format(tag),[output_size,output_size],dtype=tf.float64)\n",
    "        self.word_bias = tf.get_variable(\"word_b_{}\".format(tag),[output_size,1],dtype=tf.float64)\n",
    "        \n",
    "        # parameters of gru\n",
    "        self.W_z,self.W_r,self.W_h = tf.get_variable(\"update_w_{}\".format(tag),[output_size,input_size],dtype=tf.float64),tf.get_variable(\"reset_w_{}\".format(tag),[output_size,input_size],dtype=tf.float64),tf.get_variable(\"out_w_{}\".format(tag),[output_size,input_size],dtype=tf.float64)\n",
    "        self.U_z,self.U_r,self.U_h = tf.get_variable(\"update_u_{}\".format(tag),[output_size,output_size],dtype=tf.float64),tf.get_variable(\"reset_u_{}\".format(tag),[output_size,output_size],dtype=tf.float64),tf.get_variable(\"out_u_{}\".format(tag),[output_size,output_size],dtype=tf.float64)\n",
    "        self.b_z,self.b_r,self.b_h = tf.get_variable(\"update_b_{}\".format(tag),[output_size,1],dtype=tf.float64),tf.get_variable(\"reset_b_{}\".format(tag),[output_size,1],dtype=tf.float64),tf.get_variable(\"out_b_{}\".format(tag),[output_size,1],dtype=tf.float64)\n",
    "    \n",
    "    \"\"\" Normal GRU unit \"\"\"\n",
    "    def gru_unit(self,h_prev,steps,out=[]):\n",
    "        index = steps - self.time_steps \n",
    "        input_vector = self.input_seq[index]\n",
    "        # operations \n",
    "        z_t = tf.nn.sigmoid(tf.matmul(self.W_z,input_vector)+tf.matmul(self.U_z,h_prev)+self.b_z)\n",
    "        r_t = tf.nn.sigmoid(tf.matmul(self.W_r,input_vector)+tf.matmul(self.U_r,h_prev)+self.b_r)\n",
    "        h_t = tf.multiply(z_t,h_prev)+tf.multiply(1.-z_t,tf.nn.sigmoid(tf.matmul(self.W_h,input_vector)+tf.matmul(self.U_h,tf.multiply(r_t,h_prev))+self.b_h))\n",
    "        out.append(h_t)\n",
    "        self.time_steps -= 1\n",
    "        if self.time_steps == 0:\n",
    "            return(out)\n",
    "        else:\n",
    "            return self.gru_unit(h_prev = h_t,out = out,steps = steps) \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    it's a modified GRU !!!\n",
    "    \"\"\"\n",
    "    def mod_gru_unit(self,h_prev,steps,scalar_values,out=[],index=0):\n",
    "        input_vector = self.input_seq[index]\n",
    "        # operations\n",
    "        z_t = tf.nn.sigmoid(tf.matmul(self.W_z,input_vector)+tf.matmul(self.U_z,h_prev)+self.b_z)\n",
    "        r_t = tf.nn.sigmoid(tf.matmul(self.W_r,input_vector)+tf.matmul(self.U_r,h_prev)+self.b_r)\n",
    "        h_t = tf.multiply(z_t,h_prev)+tf.multiply(1.-z_t,tf.nn.sigmoid(tf.matmul(self.W_h,input_vector)+tf.matmul(self.U_h,tf.multiply(r_t,h_prev))+self.b_h))\n",
    "        h_t = (scalar_values[index]*h_t) + ((1-scalar_values[index])*h_prev)\n",
    "        out.append(h_t)\n",
    "        index += 1\n",
    "        if steps-index == 0:\n",
    "            return(out)\n",
    "        else:\n",
    "            return self.mod_gru_unit(h_prev = h_t,out = out,steps = steps,\n",
    "                                     scalar_values=scalar_values,index=index) \n",
    "        \n",
    "    \"\"\" It's a hybrid GRU for answer module \"\"\"\n",
    "    def answer_gru_unit(self,h_prev,word_prev,out=[]):\n",
    "        input_vector = tf.concat([word_prev,self.question],axis=0)\n",
    "        z_t = tf.nn.sigmoid(tf.matmul(self.W_z,input_vector)+tf.matmul(self.U_z,h_prev)+self.b_z)\n",
    "        r_t = tf.nn.sigmoid(tf.matmul(self.W_r,input_vector)+tf.matmul(self.U_r,h_prev)+self.b_r)\n",
    "        h_t = tf.multiply(z_t,h_prev)+tf.multiply(1.-z_t,tf.nn.sigmoid(tf.matmul(self.W_h,input_vector)+tf.matmul(self.U_h,tf.multiply(r_t,h_prev))+self.b_h))\n",
    "        word_pred = tf.nn.softmax(tf.matmul(self.word_weight,h_t)+self.word_bias)\n",
    "        out.append(word_pred)\n",
    "        self.time_steps -= 1\n",
    "        if self.time_steps == 0:\n",
    "            return(out)\n",
    "        else:\n",
    "            return self.answer_gru_unit(h_prev = h_t,word_prev = word_pred,out = out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMN_QA():\n",
    "    \"\"\"\n",
    "    the word vectors have size 100\n",
    "    the max no of words in a sentence is 120\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.MAX_P_WORDS = 120       # max no of words in a passage\n",
    "        self.WORD_VEC_LEN = 100      # word vector length\n",
    "        self.MAX_Q_WORDS = 10        # max no of words in question\n",
    "        self.MAX_NO_CONCEPTS = 5      # max no of concepts\n",
    "        \n",
    "        # variables and placeholders\n",
    "        self.Passage = tf.placeholder(shape=(self.MAX_P_WORDS,self.WORD_VEC_LEN,1),dtype = tf.float64)\n",
    "#         self.EOS_Tag = tf.placeholder(tf.float64)\n",
    "        self.Question = tf.placeholder(shape=(self.MAX_Q_WORDS,self.WORD_VEC_LEN,1),dtype = tf.float64) #sequence for question\n",
    "#         self.Answer = tf.placeholder(tf.float64)\n",
    "\n",
    "        # weigts for scalar kernel and memory vector\n",
    "        self.W_b = tf.get_variable(\"W_b\",shape=(100,100),dtype=tf.float64)\n",
    "        self.W_1 = tf.get_variable(\"W_1\",shape=(200,702),dtype=tf.float64)\n",
    "        self.b_1 = tf.get_variable(\"b_1\",shape=(200,1),dtype=tf.float64)\n",
    "        self.W_2 = tf.get_variable(\"W_2\",shape=(1,200),dtype=tf.float64)\n",
    "        self.b_2 = tf.get_variable(\"b_2\",shape=(1,1),dtype=tf.float64)\n",
    "        \n",
    "        # initial memory state\n",
    "#         self.memory = tf.placeholder(shape=(100,1),dtype=tf.float64)\n",
    "        \n",
    "        #training parameters\n",
    "        self.no_epoch = 10\n",
    "        self.df_contxt = pd.read_pickle('./input/squad_contxt.pkl') # context\n",
    "        self.df_qas = pd.read_pickle('./input/squad_qas.pkl')       # question and answers\n",
    "        \n",
    "    def scalar_gate_value(self,concept):  # debugged and perfect\n",
    "        z_vector = tf.concat((concept,self.memory),axis=0)\n",
    "        z_vector = tf.concat((z_vector,self.question),axis=0)\n",
    "        z_vector = tf.concat((z_vector,tf.multiply(concept,self.question)),axis=0)\n",
    "        z_vector = tf.concat((z_vector,tf.multiply(concept,self.memory)),axis=0)\n",
    "        z_vector = tf.concat((z_vector,concept-self.question),axis=0)\n",
    "        z_vector = tf.concat((z_vector,concept-self.memory),axis=0)\n",
    "        z_vector = tf.concat((z_vector,tf.matmul(tf.transpose(concept),tf.matmul(self.W_b,self.question))),axis=0)\n",
    "        z_vector = tf.concat((z_vector,tf.matmul(tf.transpose(concept),tf.matmul(self.W_b,self.memory))),axis=0)\n",
    "        g_1 =  tf.nn.tanh(tf.add(tf.matmul(self.W_1,z_vector),self.b_1))\n",
    "        g_scalar =  tf.nn.sigmoid(tf.add(tf.matmul(self.W_2,g_1),self.b_2))\n",
    "        return g_scalar\n",
    "        \n",
    "    def epi_mem_mod(self,concepts,no_of_iterations,mode,mem_states=[]): #debugged\n",
    "        self.scalar_vector = []\n",
    "        for i in concepts:\n",
    "            self.scalar_vector.append(self.scalar_gate_value(i))\n",
    "        \n",
    "        concepts_out = mode.mod_gru_unit(h_prev = self.memory,steps=len(concepts),scalar_values=self.scalar_vector,\n",
    "                          out=[])\n",
    "        self.memory = concepts_out[-1]\n",
    "        mem_states.append(concepts_out[-1])\n",
    "\n",
    "        if no_of_iterations != 0:\n",
    "            self.epi_mem_mod(concepts,no_of_iterations-1,\n",
    "                        mode,mem_states)\n",
    "\n",
    "        return mem_states\n",
    "    \n",
    "    def get_sentence_weight(self,q_no):\n",
    "        y = self.df_contxt['context'][self.df_qas['context_no'][q_no]]\n",
    "        c = -1\n",
    "        count = 0\n",
    "        snt_wt = np.zeros(self.MAX_NO_CONCEPTS)\n",
    "        if self.df_qas['is_impossible'][q_no]!= True:\n",
    "            print()\n",
    "            for i in TextBlob(y).sentences:\n",
    "                c += (len(i)+1)\n",
    "                if self.df_qas['Answer_start'][q_no] < c:\n",
    "                    snt_wt[count] = 1.\n",
    "                    break\n",
    "        return snt_wt\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        # question module \n",
    "        Q_module = gru_models(time_steps=self.MAX_Q_WORDS,\n",
    "                              init_state=np.zeros([self.WORD_VEC_LEN,1]),\n",
    "                              input_size=100,output_size=100,\n",
    "                              input_seq=self.Question,tag=1)\n",
    "        Q_out = Q_module.gru_unit(h_prev=np.zeros([self.WORD_VEC_LEN,1]),\n",
    "                                  steps=self.MAX_Q_WORDS,out=[])\n",
    "        \n",
    "        self.question = Q_out[-1] # final representation of question\n",
    "        #passage module\n",
    "        P_module = gru_models(time_steps=self.MAX_P_WORDS,\n",
    "                              init_state=np.zeros([self.WORD_VEC_LEN,1]),\n",
    "                              input_size=100,output_size=100,\n",
    "                              input_seq=self.Passage,tag=2)\n",
    "        P_out = P_module.gru_unit(h_prev=np.zeros([self.WORD_VEC_LEN,1]),steps=self.MAX_P_WORDS,out=[])\n",
    "        #episodic memory module\n",
    "        concepts = P_out[:3]  # selected E-O-S tags from P_out\n",
    "        epi_mod = gru_models(time_steps=2,\n",
    "                             init_state=np.zeros([self.WORD_VEC_LEN,1]),\n",
    "                             input_size=100,output_size=100,\n",
    "                             input_seq=concepts,tag=3)\n",
    "        self.mem_states = self.epi_mem_mod(concepts,10,epi_mod,[])\n",
    "        \n",
    "        self.output = self.scalar_vector\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def train_model(self):\n",
    "        pred = self.build()\n",
    "        cost = self.snt_wt - pred\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(cost)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            for i in range(self.no_epoch):\n",
    "                c = 0\n",
    "                while c<1000:\n",
    "                    q_no  = np.random.randint(0,10000)\n",
    "                    sess.run(optimizer,feed_dict={self.Passage: self.df_contxt['context'][self.df_qas['context_no'][q_no]] ,\n",
    "                                                 self.Question: self.df_qas['Question'][q_no] ,\n",
    "                                                 self.})\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        #answer module\n",
    "        A_module = gru_models(time_steps=len(self.mem_states),\n",
    "                              init_state=None,\n",
    "                              question=self.question,\n",
    "                              input_size=200,output_size=100,\n",
    "                              input_seq=self.mem_states,tag=4)\n",
    "        A_out = A_module.answer_gru_unit(h_prev=self.mem_states[-1],\n",
    "                                         word_prev=np.zeros([self.WORD_VEC_LEN,1]),\n",
    "                                         out=[]) \n",
    "        \"\"\"\n",
    "                \n",
    "#         print(A_out)       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = DMN_QA()\n",
    "    y = x.build()\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA2V(context_no,question_no):\n",
    "    passage = TextBlob(df_contxt['context'][context_no])\n",
    "    question = TextBlob(df_qas['Question'][question_no])\n",
    "    if df_qas['is_impossible'][question_no]==False:\n",
    "        answer = TextBlob(df_qas['Answer_text'][question_no])\n",
    "        beginning = passage[:df_qas['Answer_start'][question_no]]\n",
    "        end = passage[df_qas['Answer_start'][question_no]+len(answer):]  # verified\n",
    "        print(answer,'\\n',beginning,'\\n',end)\n",
    "    print(len(passage.words),'\\n',len(beginning.words),'\\n',len(answer.words),'\\n',len(end.words))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA2V(1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contxt['context'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_no = 200\n",
    "x = df_qas[['Question','context_no','q_no','Answer_start','Answer_text','is_impossible']]\n",
    "print(df_contxt['context'][x['context_no'][q_no]],'\\n\\n',\n",
    "      x['Question'][q_no],'\\n\\n',\n",
    "      x['Answer_text'][q_no],'\\n\\n',\n",
    "      x['Answer_start'][q_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = TextBlob(df_contxt['context'][x['context_no'][q_no]]).sentences\n",
    "k = len(df_contxt['context'][x['context_no'][q_no]])\n",
    "c = -1\n",
    "for i in y:\n",
    "#     print('\\n',i,len(i))\n",
    "    c += len(i)+1\n",
    "print(c,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qas.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contxt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = TextBlob('I am Prince.')\n",
    "p[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
